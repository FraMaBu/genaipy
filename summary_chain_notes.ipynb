{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions:\n",
      "Your task is to generate a summary of the text sample.\n",
      "Summarize the text sample provided below, delimited by triple backticks, in at most {max_words} words.\n",
      "\n",
      "Text sample:\n",
      "```{text}```\n",
      "\n",
      "Summarized text:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load prompt template\n",
    "from prompts.summary_prompt import SUMMARY_PROMPT_TEMPLATE\n",
    "\n",
    "prompt_tpl = SUMMARY_PROMPT_TEMPLATE\n",
    "print(prompt_tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct Preference Optimization (DPO): A Simplified Approach to Fine-tuning Large Language Models Ben Burtenshaw Artificial Intelligence in Plain English Ben Burtenshaw  · Follow  Published in Artificial Intelligence in Plain English  · 6 min read · Sep 11  Large language models (LLMs) have complexified the process of fine-tuning models in NLP. Initially, when models like ChatGPT first popped on the scene, the foremost approach involved training a reward model first and optimising the LLM policy. Reinforcement Learning from Human Feedback (RLHF) pushed the needle significantly and moved aside many long-fought challenges in NLP. However, it was hard work, requiring appropriate and relevant data. As well as complex multi-model architectures. Furthermore, the improved quality was only sometimes evident, and models acquired a tendency to imitate and hallucinate.  However, recent advancements have introduced simpler and more efficient methods. One such method is Direct Preference Optimization (DPO).  What is Direct Preference Optimization (DPO)? DPO is a method introduced to achieve precise control over LLMs. Reinforcement Learning from Human Feedback (RLHF) was based on training a Reward Model and then using Proximal Policy Optimization (PPO) to align the language model’s output with human preferences. This method, while effective, was complex and unstable.  DPO, on the other hand, treats the constrained reward maximization problem as a classification problem on human preference data. This approach is stable, efficient, and computationally lightweight. It eliminates the need for reward model fitting, extensive sampling, and hyperparameter tuning.  How Does DPO Work? The DPO pipeline can be broken down into two main stages:  1. Supervised Fine-tuning (SFT): This is the initial step where the model is fine-tuned on the dataset(s) of interest. 2. Preference Learning: After SFT, the model undergoes preference learning using preference data, ideally from the same distribution as the SFT examples.  The beauty of DPO lies in its simplicity. Instead of training a reward model first and then optimizing a policy based on that, DPO directly defines the preference loss as a function of the policy. This means that there’s no need to train a reward model first.  During the fine-tuning phase, DPO uses the LLM as a reward model. It optimizes the policy using a binary cross-entropy objective, leveraging human preference data to determine which responses are preferred and which are not. By comparing the model’s responses to the preferred ones, the policy is adjusted to enhance its performance.  Supervised Fine-tuning Supervised fine-tuning (SFT) is the first step of DPO. SFT is a specialized method where an LLM is further trained on a labelled dataset. This dataset provides a clear mapping between specific inputs and the desired outputs. The essence of SFT, especially when combined with preference learning, is to mould the model’s responses based on human-defined criteria, ensuring it aligns more closely with specific requirements.  Imagine a company looking to deploy a conversational AI to assist users in navigating their new application. While an off-the-shelf LLM like Falcon-7B might provide technically accurate answers, it might not resonate with the company’s tone or branding. For instance, if a user asks about a feature like “Collaborative Editing,” Falcon might offer a generic description. However, for a seamless user experience, the response should be user-friendly, detailed, and even offer troubleshooting tips. SFT refines the model’s outputs to ensure they’re not just accurate but also appropriate and consistent.  Understanding Preference Data in NLP Preference data is a curated set of options or alternatives related to a specific prompt. These options are then evaluated by annotators based on certain guidelines. The goal is to rank these options from the most preferred to the least preferred. This ranking provides insights into human preferences used to fine-tune models to produce outputs that align with human expectations.  The process of creating Preference Data contains a few steps:  Prompt Selection The foundation of PD is the prompt. There are various strategies to select prompts. Some might opt for a predefined set, while others might use templates to generate prompts dynamically. Another approach is to amalgamate predefined prompts with random ones sourced from databases.  Answer Selection Once the prompt is decided, the next step is to determine the answers. These answers can be generated from a specific version of a model or various checkpoints. The number of answers to be ranked can vary. While some might prefer a binary ranking system (best-worst), others might opt for a more granular approach, ranking answers on a scale, say from 1 to 5.  Annotation Guidelines It’s imperative to have clear annotation guidelines. These guidelines ensure that the ranking process is standardized and minimizes individual biases or interpretations.  Public Datasets for Preference Data Several datasets are available for those looking to dive into Preference Data. For instance:  OpenAI WebGPT Comparisons: This dataset offers 20k comparisons, each consisting of a question, a pair of model answers, and human-rated preference scores for each answer. OpenAI Summarization: This dataset provides 64k text summarization examples, inclusive of human-written responses and human-rated model responses. Reddit ELI5: Sourced from Q&A subreddits, this dataset boasts 270k examples of questions, answers, and scores. Human ChatGPT Comparison Corpus (HC3): This dataset provides 60k human answers and 27K ChatGPT answers for approximately 24K questions.\n"
     ]
    }
   ],
   "source": [
    "# Retrieve article to summarize\n",
    "\n",
    "text = input(\"Please copy article here:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instructions:\n",
      "Your task is to generate a summary of the text sample.\n",
      "Summarize the text sample provided below, delimited by triple backticks, in at most 150 words.\n",
      "\n",
      "Text sample:\n",
      "```Direct Preference Optimization (DPO): A Simplified Approach to Fine-tuning Large Language Models Ben Burtenshaw Artificial Intelligence in Plain English Ben Burtenshaw  · Follow  Published in Artificial Intelligence in Plain English  · 6 min read · Sep 11  Large language models (LLMs) have complexified the process of fine-tuning models in NLP. Initially, when models like ChatGPT first popped on the scene, the foremost approach involved training a reward model first and optimising the LLM policy. Reinforcement Learning from Human Feedback (RLHF) pushed the needle significantly and moved aside many long-fought challenges in NLP. However, it was hard work, requiring appropriate and relevant data. As well as complex multi-model architectures. Furthermore, the improved quality was only sometimes evident, and models acquired a tendency to imitate and hallucinate.  However, recent advancements have introduced simpler and more efficient methods. One such method is Direct Preference Optimization (DPO).  What is Direct Preference Optimization (DPO)? DPO is a method introduced to achieve precise control over LLMs. Reinforcement Learning from Human Feedback (RLHF) was based on training a Reward Model and then using Proximal Policy Optimization (PPO) to align the language model’s output with human preferences. This method, while effective, was complex and unstable.  DPO, on the other hand, treats the constrained reward maximization problem as a classification problem on human preference data. This approach is stable, efficient, and computationally lightweight. It eliminates the need for reward model fitting, extensive sampling, and hyperparameter tuning.  How Does DPO Work? The DPO pipeline can be broken down into two main stages:  1. Supervised Fine-tuning (SFT): This is the initial step where the model is fine-tuned on the dataset(s) of interest. 2. Preference Learning: After SFT, the model undergoes preference learning using preference data, ideally from the same distribution as the SFT examples.  The beauty of DPO lies in its simplicity. Instead of training a reward model first and then optimizing a policy based on that, DPO directly defines the preference loss as a function of the policy. This means that there’s no need to train a reward model first.  During the fine-tuning phase, DPO uses the LLM as a reward model. It optimizes the policy using a binary cross-entropy objective, leveraging human preference data to determine which responses are preferred and which are not. By comparing the model’s responses to the preferred ones, the policy is adjusted to enhance its performance.  Supervised Fine-tuning Supervised fine-tuning (SFT) is the first step of DPO. SFT is a specialized method where an LLM is further trained on a labelled dataset. This dataset provides a clear mapping between specific inputs and the desired outputs. The essence of SFT, especially when combined with preference learning, is to mould the model’s responses based on human-defined criteria, ensuring it aligns more closely with specific requirements.  Imagine a company looking to deploy a conversational AI to assist users in navigating their new application. While an off-the-shelf LLM like Falcon-7B might provide technically accurate answers, it might not resonate with the company’s tone or branding. For instance, if a user asks about a feature like “Collaborative Editing,” Falcon might offer a generic description. However, for a seamless user experience, the response should be user-friendly, detailed, and even offer troubleshooting tips. SFT refines the model’s outputs to ensure they’re not just accurate but also appropriate and consistent.  Understanding Preference Data in NLP Preference data is a curated set of options or alternatives related to a specific prompt. These options are then evaluated by annotators based on certain guidelines. The goal is to rank these options from the most preferred to the least preferred. This ranking provides insights into human preferences used to fine-tune models to produce outputs that align with human expectations.  The process of creating Preference Data contains a few steps:  Prompt Selection The foundation of PD is the prompt. There are various strategies to select prompts. Some might opt for a predefined set, while others might use templates to generate prompts dynamically. Another approach is to amalgamate predefined prompts with random ones sourced from databases.  Answer Selection Once the prompt is decided, the next step is to determine the answers. These answers can be generated from a specific version of a model or various checkpoints. The number of answers to be ranked can vary. While some might prefer a binary ranking system (best-worst), others might opt for a more granular approach, ranking answers on a scale, say from 1 to 5.  Annotation Guidelines It’s imperative to have clear annotation guidelines. These guidelines ensure that the ranking process is standardized and minimizes individual biases or interpretations.  Public Datasets for Preference Data Several datasets are available for those looking to dive into Preference Data. For instance:  OpenAI WebGPT Comparisons: This dataset offers 20k comparisons, each consisting of a question, a pair of model answers, and human-rated preference scores for each answer. OpenAI Summarization: This dataset provides 64k text summarization examples, inclusive of human-written responses and human-rated model responses. Reddit ELI5: Sourced from Q&A subreddits, this dataset boasts 270k examples of questions, answers, and scores. Human ChatGPT Comparison Corpus (HC3): This dataset provides 60k human answers and 27K ChatGPT answers for approximately 24K questions.```\n",
      "\n",
      "Summarized text:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build prompt\n",
    "from prompts.summary_prompt import build_summary_prompt \n",
    "\n",
    "prompt = build_summary_prompt(text=text, max_words=150)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Direct Preference Optimization (DPO) is a simplified approach to fine-tuning large language models (LLMs) in natural language processing (NLP). It eliminates the need for training a reward model and instead treats the problem as a classification problem on human preference data. The DPO pipeline consists of two stages: supervised fine-tuning (SFT) and preference learning. SFT involves training the model on a labeled dataset to align its responses with specific requirements. Preference learning uses human preference data to adjust the model's policy and enhance its performance. Creating preference data involves selecting prompts, determining answers, and establishing annotation guidelines. Several public datasets are available for preference data, including OpenAI WebGPT Comparisons, OpenAI Summarization, Reddit ELI5, and Human ChatGPT Comparison Corpus (HC3). DPO offers a simpler and more efficient method for fine-tuning LLMs."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai_apis.chat import get_chat_response\n",
    "from IPython.display import Markdown \n",
    "\n",
    "summary = get_chat_response(\n",
    "    prompt=prompt, temperature=0.2\n",
    "    )\n",
    "\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:\n",
      "{text}\n",
      "\n",
      "Instructions:\n",
      "Transform the provided text into the provided template enclosed by triple back ticks.\n",
      "\n",
      "Template:\n",
      "```\n",
      "{tpl}\n",
      "```\n",
      "\n",
      "Provide your response in {output_type}. Only return the final, filled template in your response.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from prompts.style_text_prompt import STYLE_TEXT_PROMPT_TEMPLATE\n",
    "\n",
    "style_prompt_template = STYLE_TEXT_PROMPT_TEMPLATE\n",
    "print(style_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [Short Title]\n",
      "\n",
      "## Executive summary\n",
      "(An exactly one-sentence summary of the main objective or purpose.)\n",
      "\n",
      "## Key points \n",
      "(bulleted list of 3-5 main points.)\n",
      "- [Key insight 1]\n",
      "- [Key insight 2]\n",
      "- [Key insight 3]\n",
      "- ... (only if needed)\n",
      "\n",
      "## Key concepts:\n",
      "(bulleted list of exactly 3-5 main concepts and entities.)\n",
      "- [Key concept/entity 1]\n",
      "- [Key concept/entity 2]\n",
      "- [Key concept/entity 3]\n",
      "- ... (only if needed)\n"
     ]
    }
   ],
   "source": [
    "# Load layout template\n",
    "from outlines.basic_summary_outline import BASIC_SUMMARY_OUTLINE\n",
    "\n",
    "tpl = BASIC_SUMMARY_OUTLINE.strip()\n",
    "print(tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text:\n",
      "Direct Preference Optimization (DPO) is a simplified approach to fine-tuning large language models (LLMs) in natural language processing (NLP). It eliminates the need for training a reward model and instead treats the problem as a classification problem on human preference data. The DPO pipeline consists of two stages: supervised fine-tuning (SFT) and preference learning. SFT involves training the model on a labeled dataset to align its responses with specific requirements. Preference learning uses human preference data to adjust the model's policy and enhance its performance. Creating preference data involves selecting prompts, determining answers, and establishing annotation guidelines. Several public datasets are available for preference data, including OpenAI WebGPT Comparisons, OpenAI Summarization, Reddit ELI5, and Human ChatGPT Comparison Corpus (HC3). DPO offers a simpler and more efficient method for fine-tuning LLMs.\n",
      "\n",
      "Instructions:\n",
      "Transform the provided text into the provided template enclosed by triple back ticks.\n",
      "\n",
      "Template:\n",
      "```\n",
      "# [Short Title]\n",
      "\n",
      "## Executive summary\n",
      "(An exactly one-sentence summary of the main objective or purpose.)\n",
      "\n",
      "## Key points \n",
      "(bulleted list of 3-5 main points.)\n",
      "- [Key insight 1]\n",
      "- [Key insight 2]\n",
      "- [Key insight 3]\n",
      "- ... (only if needed)\n",
      "\n",
      "## Key concepts:\n",
      "(bulleted list of exactly 3-5 main concepts and entities.)\n",
      "- [Key concept/entity 1]\n",
      "- [Key concept/entity 2]\n",
      "- [Key concept/entity 3]\n",
      "- ... (only if needed)\n",
      "```\n",
      "\n",
      "Provide your response in markdown. Only return the final, filled template in your response.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build style prompt\n",
    "from prompts.style_text_prompt import build_style_text_prompt\n",
    "\n",
    "style_prompt = build_style_text_prompt(text=summary, tpl=tpl)\n",
    "print(style_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "# Direct Preference Optimization (DPO)\n",
       "\n",
       "## Executive summary\n",
       "DPO is a simplified approach to fine-tuning large language models (LLMs) in natural language processing (NLP) by treating the problem as a classification problem on human preference data.\n",
       "\n",
       "## Key points\n",
       "- DPO eliminates the need for training a reward model and instead uses human preference data for fine-tuning LLMs.\n",
       "- The DPO pipeline consists of two stages: supervised fine-tuning (SFT) and preference learning.\n",
       "- SFT involves training the model on a labeled dataset to align its responses with specific requirements.\n",
       "- Preference learning uses human preference data to adjust the model's policy and enhance its performance.\n",
       "- Several public datasets are available for preference data, including OpenAI WebGPT Comparisons, OpenAI Summarization, Reddit ELI5, and Human ChatGPT Comparison Corpus (HC3).\n",
       "\n",
       "## Key concepts:\n",
       "- Direct Preference Optimization (DPO)\n",
       "- Large Language Models (LLMs)\n",
       "- Natural Language Processing (NLP)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "styled_summary = get_chat_response(prompt=style_prompt, temperature=0) \n",
    "Markdown(styled_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
