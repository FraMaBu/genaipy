{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Creating Content from Text Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "This notebook demoes how a workflow of sequential requests to large language models can be leveraged to create multiple pieces of content from an arbitrary text sample.\n",
    "\n",
    "**This workflow consists of three sequential steps:**\n",
    "1. Generate a summary from an arbitrary text sample\n",
    "2. Structure the summary according to a user-defined outline.\n",
    "3. Create a social media post from structured summary with custom persona.\n",
    "\n",
    "**Requirements:**\n",
    "- This demo uses the OpenAI Chat API and, thus, requires an OpenAI API key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [1/3]: Generate Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step of the workflow, we provide the input text sample to `GPT-3.5-turbo` and request it to generate a 150 words long summary. This step serves as the foundation for the following steps.\n",
    "\n",
    "For this demo, we use the first half of an [article about PEFT](https://huggingface.co/blog/peft) from the huggingface blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Parameter-Efficient Fine-Tuning (PEFT) approaches aim to address the challenges of training and deploying large language models (LLMs) on low-resource hardware. These approaches involve fine-tuning a small number of model parameters while freezing most of the pretrained LLM's parameters, reducing computational and storage costs. PEFT has shown to be effective in low-data regimes and out-of-domain scenarios, and can be applied to various modalities such as image classification. It also offers portability, allowing users to tune models using PEFT methods to obtain small checkpoints compared to the large checkpoints of full fine-tuning. These small trained weights can be added on top of the pretrained LLM, enabling the same model to be used for multiple tasks without replacing the entire model. Overall, PEFT approaches provide comparable performance to full fine-tuning with a smaller number of trainable parameters."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "from openai_apis.chat import get_chat_response\n",
    "from prompts.generate_summaries import build_summary_prompt\n",
    "\n",
    "text = input(\"Please copy article here:\")\n",
    "\n",
    "prompt = build_summary_prompt(text=text, max_words=150)\n",
    "\n",
    "summary = get_chat_response(prompt=prompt, temperature=0.2)\n",
    "Markdown(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [2/3]: Structure Summary with Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step of the workflow, we use the generated summary and utilize `GPT-3.5-turbo` to structure it according to a custom outline. Notably, this structured outline is completely customizable to the requirements of the use case (e.g., meeting minutes, reports, etc.).\n",
    "\n",
    "Here, we opted for a basic outline for structuring summaries displayed below. Since the structured summary is in `Markdown`, it can be directly saved and displayed in many text editors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# [Short Title]\n",
      "\n",
      "## Summary\n",
      "(Exactly 1 sentence long summary of the main topic and objective.)\n",
      "\n",
      "## Key points \n",
      "(bulleted list of 3-5 main points.)\n",
      "- [Key insight 1]\n",
      "- [Key insight 2]\n",
      "- [Key insight 3]\n",
      "- ... (only if needed)\n",
      "\n",
      "## Key concepts\n",
      "(bulleted list of exactly 3-5 main concepts and entities.)\n",
      "- [Key concept/entity 1]\n",
      "- [Key concept/entity 2]\n",
      "- [Key concept/entity 3]\n",
      "- ... (only if needed)\n"
     ]
    }
   ],
   "source": [
    "# Load structured outline\n",
    "\n",
    "from outlines.basic_summary_outline import BASIC_SUMMARY_OUTLINE\n",
    "\n",
    "tpl = BASIC_SUMMARY_OUTLINE.strip()\n",
    "print(tpl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```\n",
       "# Parameter-Efficient Fine-Tuning (PEFT) Approaches\n",
       "\n",
       "## Summary\n",
       "PEFT approaches aim to address the challenges of training and deploying large language models (LLMs) on low-resource hardware by fine-tuning a small number of additional model parameters while freezing most of the pretrained LLM parameters, reducing computational and storage costs and achieving comparable performance to full fine-tuning.\n",
       "\n",
       "## Key points\n",
       "- PEFT approaches only fine-tune a small number of additional model parameters while freezing most of the pretrained LLM parameters.\n",
       "- PEFT reduces computational and storage costs and overcomes issues like catastrophic forgetting.\n",
       "- PEFT approaches have shown to be better than fine-tuning in low-data regimes and generalize well to out-of-domain scenarios.\n",
       "\n",
       "## Key concepts\n",
       "- Parameter-Efficient Fine-Tuning (PEFT)\n",
       "- Large Language Models (LLMs)\n",
       "- Catastrophic forgetting\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate structured summary\n",
    "\n",
    "from prompts.style_texts import build_style_prompt\n",
    "\n",
    "style_prompt = build_style_prompt(text=summary, tpl=tpl)\n",
    "\n",
    "styled_summary = get_chat_response(prompt=style_prompt, temperature=0) \n",
    "Markdown(styled_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [3/3]: Generate Social Media Post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third step of the workflow, we generate a social media post from the structured summary using `GPT-4-turbo`. \n",
    "Notably, we set a custom role for the LLM via the system message to tailor the style and tone of the post.\n",
    "\n",
    "In this case, we set the LLM's role to a  professional content creator on Linkedin excelling at communicating complex AI topics in a simple way. Feel free to change the system message below, to explore different types of social media posts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "You are an expert content creator on LinkedIn. You excell at communicating complex topics in Generative AI in a simple and professional way."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define custom role\n",
    "\n",
    "system_message = \"You are an expert content creator on LinkedIn. You excell at communicating complex topics in Generative AI in a simple and professional way.\"\n",
    "Markdown(system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Navigating the world of AI, we're constantly seeking ways to optimize our models. Today, I'm delving into the realm of Parameter-Efficient Fine-Tuning (PEFT) approaches for Large Language Models (LLMs).\n",
       "\n",
       "Here's why PEFT is a game-changer:\n",
       "- ðŸŽ¯ **Focus**: It hones in on a fraction of model parameters, reducing the heavy lifting without sacrificing performance.\n",
       "- ðŸ’¾ **Efficiency**: By not retraining the entire model, we slash computational and storage demands.\n",
       "- ðŸ§  **Memory**: PEFT helps prevent catastrophic forgetting, a common hurdle in AI training.\n",
       "\n",
       "PEFT isn't just about doing more with less; it's about smart adaptation, especially in low-data scenarios and when generalizing to new domains. It's an exciting time for AI enthusiasts and professionals alike as we explore these frontiers of model efficiency. #AI #MachineLearning #LanguageModels #PEFT"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate LinkedIn post\n",
    "\n",
    "from prompts.generate_posts import build_post_prompt\n",
    "\n",
    "generate_prompt = build_post_prompt(text=styled_summary, max_words=150)\n",
    "\n",
    "post = get_chat_response(\n",
    "    prompt=generate_prompt,\n",
    "    sys_message=system_message,\n",
    "    temperature=0.5,\n",
    "    model=\"gpt-4-1106-preview\"\n",
    "    )\n",
    "\n",
    "Markdown(post)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-apps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
