{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Summarizing PDFs with Map-Reduce Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "This notebook demoes a workflow leveraging the `Map-Reduce` technique to generate coherent summaries of PDF documents.\n",
    "\n",
    "The workflow consists of three sequential steps:\n",
    "1. **Load and split PDF**: Load the desired PDF and create text chunks for each page.\n",
    "2. **Map-Step**: For each page, generate a concise summary (~150 words) using `GPT-3.5-turbo`.\n",
    "3. **Reduce-Step**: Create final, consolidated summary using `GPT-4-turbo `from set of summaries generated in `Map-Step`.\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "This modular approach, which breaks down a complex task into manageable sub-tasks, offers several advantages over directly generating a summary from the whole PDF:\n",
    "- **Handling of Large Data Volumes**: Enables efficient handling of larger documents by breaking them down into manageable pages, which allows for parallel processing.\n",
    "- **Cost Savings with Cheaper LLMs**: Allows for the use of more cost-effective LLMs for generating intermediate summaries in the `Map-Step`, reducing overall operational costs while maintaining effectiveness.\n",
    "- **Improved Summary Quality**: Each page is summarized individually, leading to more nuanced and accurate summaries, with the flexibility to customize and adjust the process for different use cases.\n",
    "\n",
    "**Disclaimer:**\n",
    "\n",
    "This notebook is strictly for educational purposes. Users should employ it responsibly, ensuring accuracy, respecting privacy and copyright, and avoiding the dissemination of misinformation. Always consider the ethical implications and context of the generated content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load dependencies and set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an inital step, we load the required dependencies and set the parameters for the `Map-Reduce` workflow. In this demo, we summarize the main content of the seminal `\"Attention Is All You Need\"` paper by Google Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.chdir('d:/genai/GenAIPy/') # Change directory to root\n",
    "\n",
    "from genaipy.extractors.pdf import extract_pages_text\n",
    "from genaipy.openai_apis.chat import get_chat_response\n",
    "from genaipy.prompts.generate_summaries import build_summary_prompt, REDUCE_SUMMARY_PROMPT_TPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging settings\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# PDF settings\n",
    "PDF_URL = \"demos/data/transformers_attention_paper.pdf\" # https://arxiv.org/abs/1706.03762\n",
    "START = 1\n",
    "END = 10 # exclude references and appendix\n",
    "\n",
    "# LLM settings\n",
    "MAP_LLM = \"gpt-3.5-turbo\"\n",
    "REDUCE_LLM = \"gpt-4-1106-preview\"\n",
    "SYS_MESSAGE = \"You are a generative AI expert. You explain complex AI concepts in simple language so regular users can understand your answers.\"\n",
    "MAP_MAX_WORDS = 150\n",
    "REDUCE_MAX_WORDS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [1/3]: Load and split PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we load the PDF article and split the text content of each page in separate chunks. This enables us to generate an individual summary per page in the following `Map-Step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded text from 10 PDF pages.\n"
     ]
    }
   ],
   "source": [
    "pages = extract_pages_text(pdf_path=PDF_URL, start_page=START, end_page=END)\n",
    "print(f\"Successfully loaded text from {len(pages)} PDF pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [2/3]: Map-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, we execute the mapping loop of the `Map-Reduce` technique. Specifically, we generate a summary of each page's text content with `GPT-3.5-turbo` and store the resulting intermediate summaries in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "451643a7e7b9473598b3a12b695ca4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Map Summaries:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:13:44,413 - INFO - Successfully completed Chat API request. Total token usage: 889\n",
      "2023-11-27 11:13:44,414 - INFO - Map Summary #1: The text sample introduces the concept of the Transformer, a network architecture based solely on attention mechanisms. Unlike traditional models that use recurrent or convolutional neural networks, the Transformer connects an encoder and a decoder through attention mechanisms. The authors conducted experiments on machine translation tasks and found that the Transformer outperformed existing models in terms of quality, parallelizability, and training time. It achieved a BLEU score of 28.4 on the English-to-German translation task and established a state-of-the-art BLEU score of 41.8 on the English-to-French translation task. The authors also demonstrated that the Transformer can generalize well to other tasks, such as English constituency parsing.\n",
      "2023-11-27 11:13:53,540 - INFO - Successfully completed Chat API request. Total token usage: 1075\n",
      "2023-11-27 11:13:53,541 - INFO - Map Summary #2: The text sample introduces the concept of the Transformer model, which is a type of neural network architecture that does not rely on recurrent connections or convolutional layers. Instead, it uses self-attention mechanisms to capture dependencies between input and output sequences. This allows for more parallelization and reduces the computational complexity compared to traditional recurrent models. The Transformer model has been shown to achieve state-of-the-art performance in tasks like language modeling and machine translation. It consists of an encoder-decoder structure with stacked self-attention and fully connected layers. The model can be trained efficiently and has the ability to capture global dependencies between sequences.\n",
      "2023-11-27 11:14:02,811 - INFO - Successfully completed Chat API request. Total token usage: 601\n",
      "2023-11-27 11:14:02,812 - INFO - Map Summary #3: The text sample describes the architecture of the Transformer model. The model consists of an encoder and a decoder. The encoder has multiple layers with self-attention and feed-forward networks. There are residual connections and layer normalization used in each layer. The decoder also has multiple layers and adds a sub-layer for attention over the encoder's output. There's a modification in the decoder to prevent positions from attending to subsequent positions. The attention function in the model maps a query and a set of key-value pairs to an output, using a compatibility function. Overall, the Transformer model is designed to handle complex language tasks with the help of self-attention and encoder-decoder architectures.\n",
      "2023-11-27 11:14:15,126 - INFO - Successfully completed Chat API request. Total token usage: 747\n",
      "2023-11-27 11:14:15,127 - INFO - Map Summary #4: The text sample discusses two concepts related to attention mechanisms in artificial intelligence: Scaled Dot-Product Attention and Multi-Head Attention. Scaled Dot-Product Attention calculates the dot products of queries and keys and applies a softmax function to obtain weights on the values. It is efficient and faster in practice due to optimized matrix multiplication code. Multi-Head Attention involves linearly projecting queries, keys, and values multiple times and performing attention functions in parallel. The outputs are concatenated and projected again to obtain the final values. These concepts are important in deep learning models to enhance the understanding and processing of information.\n",
      "2023-11-27 11:14:25,772 - INFO - Successfully completed Chat API request. Total token usage: 952\n",
      "2023-11-27 11:14:25,773 - INFO - Map Summary #5: The provided text discusses the concept of multi-head attention in the Transformer model. Multi-head attention allows the model to attend to different information from various positions simultaneously. By using multiple attention heads, the model can better capture diverse information. The text also mentions the application of multi-head attention in the encoder-decoder layers, self-attention layers in the encoder, and self-attention layers in the decoder. Additionally, the text describes the use of position-wise feed-forward networks and embeddings to convert input and output tokens to vectors. It also mentions the utilization of positional encoding to incorporate sequence order information.\n",
      "2023-11-27 11:14:39,798 - INFO - Successfully completed Chat API request. Total token usage: 1024\n",
      "2023-11-27 11:14:39,799 - INFO - Map Summary #6: The text provides an analysis of self-attention layers in comparison to recurrent and convolutional layers. It discusses the computational complexity, amount of parallelization, and path length between long-range dependencies. Self-attention layers require a constant number of sequentially executed operations, making them faster than recurrent layers for smaller sequence lengths. However, for longer sequences, self-attention can be restricted to a neighborhood size to improve computational performance. The text also mentions the use of positional encodings to enable the model to learn relative positions. The choice of positional encodings and their impact on the results is discussed. Overall, self-attention layers offer advantages in terms of computational efficiency and learning long-range dependencies.\n",
      "2023-11-27 11:14:47,154 - INFO - Successfully completed Chat API request. Total token usage: 984\n",
      "2023-11-27 11:14:47,155 - INFO - Map Summary #7: The text discusses the use of self-attention in neural networks and its potential benefits. Self-attention allows the network to focus on different parts of the input sequence, increasing interpretability and performance. The authors also mention the use of convolutional layers, which may require multiple layers to connect all input and output positions. However, separable convolutions can decrease complexity. The models were trained on large datasets using batching and hardware with GPUs. The Adam optimizer was used with a varying learning rate throughout training. Regularization techniques, such as dropout, were applied to improve performance. Overall, the authors present their findings and the approach they took in their model.\n",
      "2023-11-27 11:14:56,079 - INFO - Successfully completed Chat API request. Total token usage: 1173\n",
      "2023-11-27 11:14:56,080 - INFO - Map Summary #8: The provided text sample describes the performance of the Transformer model on machine translation tasks. The Transformer model achieves better BLEU scores compared to previous state-of-the-art models for English-to-German and English-to-French translations. The big Transformer model outperforms all previously reported models and ensembles with a new state-of-the-art BLEU score of 28.4 for English-to-German and a BLEU score of 41.0 for English-to-French translations. The training cost for the Transformer model is significantly lower than other competitive models. The text also mentions variations in the base model of the Transformer, such as different attention heads and dimensions, and their impact on translation performance.\n",
      "2023-11-27 11:15:05,107 - INFO - Successfully completed Chat API request. Total token usage: 1066\n",
      "2023-11-27 11:15:05,108 - INFO - Map Summary #9: The provided text sample contains information about variations on the Transformer architecture, such as different model configurations, parameters, and their impact on performance metrics like perplexity and BLEU score. It also discusses the generalizability of the Transformer to English constituency parsing, comparing its performance with other models. Notable findings include the influence of attention key size on model quality, the benefits of using bigger models and dropout for avoiding overfitting, and the similarity in results between sinusoidal and learned positional embeddings. Overall, the Transformer architecture performs well across different tasks, showcasing its versatility and effectiveness in natural language processing.\n",
      "2023-11-27 11:15:14,102 - INFO - Successfully completed Chat API request. Total token usage: 1138\n",
      "2023-11-27 11:15:14,104 - INFO - Map Summary #10: The text sample discusses the Transformer model, which is a sequence transduction model based entirely on attention. The Transformer replaces recurrent layers typically used in encoder-decoder architectures with multi-headed self-attention. The author trained a 4-layer Transformer on the Wall Street Journal (WSJ) portion of the Penn Treebank and achieved better results than previously reported models in small-data regimes. The Transformer outperforms other models, even the Berkeley-Parser, when trained only on the WSJ training set. The model can be trained faster than architectures based on recurrent or convolutional layers and achieves state-of-the-art performance in translation tasks. The authors plan to extend the Transformer to other tasks and investigate attention mechanisms for handling large inputs and outputs such as images, audio, and video.\n"
     ]
    }
   ],
   "source": [
    "map_summaries = []\n",
    "for page in tqdm(pages, desc=\"Generating Map Summaries\"):\n",
    "    try:\n",
    "        map_prompt = build_summary_prompt(pages[page][\"content\"], max_words=MAP_MAX_WORDS)\n",
    "        summary = get_chat_response(map_prompt, sys_message= SYS_MESSAGE, model=MAP_LLM)\n",
    "        map_summaries.append(summary)\n",
    "        logging.info(\"Map Summary #%d: %s\", page, summary)\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occured while generating summary #%d: %s\", page, summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [3/3]: Reduce-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of the workflow, we combine the individual intermediate summaries, so we can distill them into a cohesive summary in the `Reduce-Step` with `GPT-4-turbo`. As we can see, the resulting final summary gives a comprehensive overview of the paper without omitting important information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text sample introduces the concept of the Transformer, a network architecture based solely on attention mechanisms. Unlike traditional models that use recurrent or convolutional neural networks, the Transformer connects an encoder and a decoder through attention mechanisms. The authors conducted experiments on machine translation tasks and found that the Transformer outperformed existing models in terms of quality, parallelizability, and training time. It achieved a BLEU score of 28.4 on the English-to-German translation task and established a state-of-the-art BLEU score of 41.8 on the English-to-French translation task. The authors also demonstrated that the Transformer can generalize well to other tasks, such as English constituency parsing.\n",
      "The text sample introduces the concept of the Transformer model, which is a type of neural network architecture that does not rely on recurrent connections or convolutional layers. Instead, it uses self-attention mechanisms to capture dependencies between input and output sequences. This allows for more parallelization and reduces the computational complexity compared to traditional recurrent models. The Transformer model has been shown to achieve state-of-the-art performance in tasks like language modeling and machine translation. It consists of an encoder-decoder structure with stacked self-attention and fully connected layers. The model can be trained efficiently and has the ability to capture global dependencies between sequences.\n",
      "The text sample describes the architecture of the Transformer model. The model consists of an encoder and a decoder. The encoder has multiple layers with self-attention and feed-forward networks. There are residual connections and layer normalization used in each layer. The decoder also has multiple layers and adds a sub-layer for attention over the encoder's output. There's a modification in the decoder to prevent positions from attending to subsequent positions. The attention function in the model maps a query and a set of key-value pairs to an output, using a compatibility function. Overall, the Transformer model is designed to handle complex language tasks with the help of self-attention and encoder-decoder architectures.\n",
      "The text sample discusses two concepts related to attention mechanisms in artificial intelligence: Scaled Dot-Product Attention and Multi-Head Attention. Scaled Dot-Product Attention calculates the dot products of queries and keys and applies a softmax function to obtain weights on the values. It is efficient and faster in practice due to optimized matrix multiplication code. Multi-Head Attention involves linearly projecting queries, keys, and values multiple times and performing attention functions in parallel. The outputs are concatenated and projected again to obtain the final values. These concepts are important in deep learning models to enhance the understanding and processing of information.\n",
      "The provided text discusses the concept of multi-head attention in the Transformer model. Multi-head attention allows the model to attend to different information from various positions simultaneously. By using multiple attention heads, the model can better capture diverse information. The text also mentions the application of multi-head attention in the encoder-decoder layers, self-attention layers in the encoder, and self-attention layers in the decoder. Additionally, the text describes the use of position-wise feed-forward networks and embeddings to convert input and output tokens to vectors. It also mentions the utilization of positional encoding to incorporate sequence order information.\n",
      "The text provides an analysis of self-attention layers in comparison to recurrent and convolutional layers. It discusses the computational complexity, amount of parallelization, and path length between long-range dependencies. Self-attention layers require a constant number of sequentially executed operations, making them faster than recurrent layers for smaller sequence lengths. However, for longer sequences, self-attention can be restricted to a neighborhood size to improve computational performance. The text also mentions the use of positional encodings to enable the model to learn relative positions. The choice of positional encodings and their impact on the results is discussed. Overall, self-attention layers offer advantages in terms of computational efficiency and learning long-range dependencies.\n",
      "The text discusses the use of self-attention in neural networks and its potential benefits. Self-attention allows the network to focus on different parts of the input sequence, increasing interpretability and performance. The authors also mention the use of convolutional layers, which may require multiple layers to connect all input and output positions. However, separable convolutions can decrease complexity. The models were trained on large datasets using batching and hardware with GPUs. The Adam optimizer was used with a varying learning rate throughout training. Regularization techniques, such as dropout, were applied to improve performance. Overall, the authors present their findings and the approach they took in their model.\n",
      "The provided text sample describes the performance of the Transformer model on machine translation tasks. The Transformer model achieves better BLEU scores compared to previous state-of-the-art models for English-to-German and English-to-French translations. The big Transformer model outperforms all previously reported models and ensembles with a new state-of-the-art BLEU score of 28.4 for English-to-German and a BLEU score of 41.0 for English-to-French translations. The training cost for the Transformer model is significantly lower than other competitive models. The text also mentions variations in the base model of the Transformer, such as different attention heads and dimensions, and their impact on translation performance.\n",
      "The provided text sample contains information about variations on the Transformer architecture, such as different model configurations, parameters, and their impact on performance metrics like perplexity and BLEU score. It also discusses the generalizability of the Transformer to English constituency parsing, comparing its performance with other models. Notable findings include the influence of attention key size on model quality, the benefits of using bigger models and dropout for avoiding overfitting, and the similarity in results between sinusoidal and learned positional embeddings. Overall, the Transformer architecture performs well across different tasks, showcasing its versatility and effectiveness in natural language processing.\n",
      "The text sample discusses the Transformer model, which is a sequence transduction model based entirely on attention. The Transformer replaces recurrent layers typically used in encoder-decoder architectures with multi-headed self-attention. The author trained a 4-layer Transformer on the Wall Street Journal (WSJ) portion of the Penn Treebank and achieved better results than previously reported models in small-data regimes. The Transformer outperforms other models, even the Berkeley-Parser, when trained only on the WSJ training set. The model can be trained faster than architectures based on recurrent or convolutional layers and achieves state-of-the-art performance in translation tasks. The authors plan to extend the Transformer to other tasks and investigate attention mechanisms for handling large inputs and outputs such as images, audio, and video.\n"
     ]
    }
   ],
   "source": [
    "# Process generated map summaries for reduce step\n",
    "\n",
    "#map_summaries = ['The text introduces a new network architecture called the Transformer for sequence transduction tasks, such as machine translation. Unlike existing models that use recurrent or convolutional neural networks, the Transformer is solely based on attention mechanisms, eliminating the need for recurrence and convolutions. The experiments show that the Transformer models outperform existing models in terms of quality, parallelizability, and training time. For example, the Transformer achieves a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, surpassing previous results by more than 2 BLEU. It also achieves a state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French translation task. Additionally, the Transformer shows good generalization to other tasks, such as English constituency parsing.', 'The text discusses the limitations of recurrent models in language modeling and machine translation tasks, such as slow sequential computation and difficulty in learning dependencies between distant positions. It introduces the Transformer model, which relies on an attention mechanism to establish global dependencies between input and output. The Transformer allows for more parallelization, achieves state-of-the-art translation quality in a short training time, and reduces the complexity of relating signals from different positions. The model architecture includes an encoder-decoder structure with stacked self-attention and fully connected layers. The Transformer is the first transduction model to use self-attention exclusively, without relying on recurrent networks or convolution.', \"The text describes the architecture and components of the Transformer model, which is used in natural language processing tasks. The architecture consists of an encoder and decoder stack. The encoder has multiple layers, each containing a self-attention mechanism and a feed-forward network. The decoder stack is similar, with an additional sub-layer for attention over the encoder's output. The self-attention mechanism calculates weights for each value based on the compatibility of the query with the corresponding key. Residual connections and layer normalization are used to improve information flow and prevent positions from attending to subsequent positions. The model produces outputs of dimension 512.\", 'The text discusses two key components of attention mechanisms in artificial intelligence. The first is Scaled Dot-Product Attention, which computes the dot products of queries and keys, divides them by a scaling factor, and applies a softmax function to obtain weights on values. Scaled Dot-Product Attention is faster and more space-efficient than additive attention, which uses a feed-forward network. The second component is Multi-Head Attention, which involves linearly projecting queries, keys, and values multiple times before performing the attention function. The resulting output values are concatenated and projected again to obtain the final values. This technique improves the performance of attention mechanisms when working with high-dimensional data.', 'The multi-head attention mechanism allows a model to pay attention to different information at different positions. It combines attention heads to achieve this. The Transformer model uses multi-head attention in three ways: encoder-decoder attention, self-attention in the encoder, and self-attention in the decoder. The encoder-decoder attention allows every position in the decoder to attend to all positions in the input sequence. Self-attention in the encoder and decoder allows each position to attend to all positions in the previous layer. The model also includes position-wise feed-forward networks, which apply a fully connected network to each position separately and identically. The model uses embeddings to convert input and output tokens to vectors of a specified dimension. Positional encoding is used to inject information about the order of the sequence into the model.', 'The text discusses the complexity and efficiency of different layer types in sequence transduction tasks, such as self-attention, recurrent, and convolutional layers. Self-attention layers have a constant number of sequential operations and shorter maximum path lengths compared to recurrent layers. They are faster when the sequence length is smaller than the representation dimensionality. To improve performance with longer sequences, self-attention can be restricted to a neighborhood size. The text also mentions the use of positional encodings to incorporate the position information of tokens in the sequence. Sine and cosine functions of different frequencies are used as positional encodings. The choice between learned and fixed positional encodings does not significantly impact the results. The rationale behind using self-attention layers is their computational efficiency, parallelizability, and ability to learn long-range dependencies.', 'The text discusses various aspects related to the training and complexity of models in the context of convolutional and self-attention layers. It mentions that using a single convolutional layer may not connect all input and output positions and requires multiple layers to do so. However, separable convolutions decrease complexity considerably. The text also highlights the potential interpretability of models achieved through self-attention. \\n\\nIn terms of training, the models were trained on a specific dataset using byte-pair encoding, and the training batches consisted of approximately 25,000 source tokens and 25,000 target tokens. The text then describes the hardware, schedule, optimizer, and regularization techniques used during training.', 'The Transformer model achieves better BLEU scores than previous state-of-the-art models on English-to-German and English-to-French translation tests, while also having a lower training cost. The big Transformer model performs particularly well, outperforming all previous models on both translation tasks. This model achieved a BLEU score of 28.4 on English-to-German translation and a BLEU score of 41.0 on English-to-French translation. The base model also surpasses previous models and ensembles. The Transformer model uses label smoothing during training, which improves accuracy and BLEU score but hurts perplexity. The results are summarized in Table 2, comparing translation quality and training costs to other model architectures. Different variations of the base model were also evaluated, and the importance of attention heads and dimensions was highlighted.', \"The text provides a table showing variations on the Transformer architecture and their impact on model performance in English-to-German translation. It also mentions that bigger models with dropout tend to perform better. Additionally, the text mentions that replacing sinusoidal positional encoding with learned positional embeddings does not significantly affect the model's performance. The text then briefly mentions that the Transformer architecture also shows good results in English constituency parsing, performing well compared to other models and approaches.\", 'The text discusses the performance of the Transformer model, which is a sequence transduction model based entirely on attention. It explains that the Transformer outperforms recurrent neural network (RNN) models and the Berkeley-Parser even with small amounts of training data. The paper presents results from experiments using the Wall Street Journal dataset and high-confidence and BerkleyParser corpora. The model achieves state-of-the-art results in translation tasks, surpassing previously reported models. The paper also mentions plans to extend the Transformer to handle other input/output modalities, such as images, audio, and video. Code used to train and evaluate the models is available on GitHub.']\n",
    "string = \"\\n\".join(map_summaries)\n",
    "text = string.replace(\"\\n\\n\", \"\") # clean potential double newlines in joined map summaries\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:23:48,243 - INFO - Successfully completed Chat API request. Total token usage: 1739\n",
      "2023-11-27 11:23:48,244 - INFO - Final Reduce Summary:\n",
      "### Overview of the Transformer Model\n",
      "\n",
      "#### Architecture:\n",
      "- The Transformer is an innovative neural network design for handling sequence-to-sequence tasks, such as language translation.\n",
      "- It comprises two main components: an encoder to process the input and a decoder to generate the output.\n",
      "- Multiple layers within both the encoder and decoder use self-attention and feed-forward networks.\n",
      "\n",
      "#### Self-Attention Mechanisms:\n",
      "- Self-attention enables the model to weigh the importance of different words within the input sequence, regardless of their position.\n",
      "- It replaces recurrent and convolutional layers, allowing for higher computational efficiency and better handling of long-range dependencies.\n",
      "- Multi-Head Attention is a key feature that allows the model to focus on different segments simultaneously, capturing a wider range of information.\n",
      "\n",
      "#### Advantages:\n",
      "- The Transformer favors parallel processing, making it faster to train on GPUs compared to models with recurrent layers.\n",
      "- It showcases state-of-the-art performance in translation tasks, offering better BLEU scores, a metric for evaluating translated text's quality.\n",
      "- The model is adaptable and has generalized well to other language tasks like constituency parsing.\n",
      "\n",
      "#### Training and Performance:\n",
      "- Utilization of novel training techniques like the Adam optimizer, varied learning rates, and regularization methods like dropout to enhance performance.\n",
      "- Trained with large datasets, it works more efficiently with hardware acceleration from GPUs.\n",
      "- Configurations such as attention heads and positional encoding techniques contribute significantly to the model's efficacy.\n",
      "\n",
      "#### Future Directions:\n",
      "- The Transformer is not just limited to text; there's potential for future applications in processing images, audio, and video.\n",
      "- There is an ongoing exploration to improve the model's ability to manage larger inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "# Execute reduce step to generate final summary\n",
    "\n",
    "try:\n",
    "    reduce_prompt = build_summary_prompt(text=text, max_words=REDUCE_MAX_WORDS, template=REDUCE_SUMMARY_PROMPT_TPL)\n",
    "    final_summary = get_chat_response(prompt=reduce_prompt, sys_message=SYS_MESSAGE, model=REDUCE_LLM, max_tokens=1024)\n",
    "    logging.info(\"Final Reduce Summary:\\n%s\", final_summary)\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occured while generating final summary: {e}\")\n",
    "else:\n",
    "    output = {\"final_summary\": final_summary, \"map_summaries\": map_summaries}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 11:44:54,971 - INFO - Successfully completed Chat API request. Total token usage: 2065\n",
      "2023-11-27 11:44:54,973 - INFO - Final Reduce Summary:\n",
      "# Final Summary of the Transformer Model\n",
      "\n",
      "## Introduction to the Transformer\n",
      "The Transformer is an innovative neural network architecture that has significantly impacted the field of natural language processing (NLP). Unlike traditional models that relied on recurrent or convolutional layers, the Transformer solely utilizes attention mechanisms to handle sequences of data.\n",
      "\n",
      "## Core Components of the Transformer\n",
      "- **Encoder and Decoder:** The model is structured with an encoder that processes the input sequence and a decoder that produces the output sequence.\n",
      "- **Self-Attention Mechanism:** At the heart of the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input when producing a specific part of the output.\n",
      "- **Scaled Dot-Product Attention:** This attention function calculates the compatibility of different inputs by taking queries and keys, computing their dot product, and scaling down the results before applying a softmax function to derive weights for the values.\n",
      "- **Multi-Head Attention:** Involving multiple 'heads' of attention, this feature allows the model to capture different aspects of information from the same input simultaneously, increasing the richness of the representation.\n",
      "- **Positional Encodings:** Since the model lacks recurrence and convolutions, positional encodings provide information about the order of the sequence elements.\n",
      "\n",
      "## Advantages of the Transformer\n",
      "- **Parallelization:** The architecture of the Transformer enables more efficient training by allowing for computations to be done in parallel, significantly reducing training time.\n",
      "- **Performance:** Transformers have achieved state-of-the-art results in various tasks like machine translation and language modeling. They boast impressive BLEU scores on English-to-German and English-to-French translation tasks.\n",
      "- **Global Dependencies:** The model can capture long-range dependencies in sequences better than prior methods, enhancing the understanding of input sequences.\n",
      "\n",
      "## Efficiency and Training\n",
      "- **Computational Complexity:** Self-attention has a lower computational complexity compared to recurrent or convolutional layers for shorter sequences, although for longer sequences, optimizations may be needed.\n",
      "- **Optimized Learning:** The Transformer utilizes the Adam optimizer, varying learning rates, and regularization techniques like dropout to improve training efficiency and prevent overfitting.\n",
      "- **Hardware Utilization:** Leveraging GPUs during training allows for handling large datasets effectively, further demonstrating the Transformer's efficiency.\n",
      "\n",
      "## Versatility and Generalizability\n",
      "- **Variety of Tasks:** Beyond translation, Transformers have also been successfully applied to tasks such as constituency parsing, showcasing its broad applicability in NLP.\n",
      "- **Model Variations:** Different configurations of the Transformer (e.g., number of attention heads, size of attention keys, model dimensions) can impact performance, allowing for customization based on specific objectives.\n",
      "\n",
      "## Potential and Future Directions\n",
      "- **Handling Large Inputs and Outputs:** There is ongoing research into adapting the Transformer to manage large datasets, including images, audio, and video.\n",
      "- **Continued Evolution:** As the model is further developed, there are plans to extend its application to a wider array of tasks and explore deeper into the mechanics of attention mechanisms.\n",
      "\n",
      "In conclusion, the Transformer model has revolutionized the handling of sequence data in artificial intelligence by leveraging attention mechanisms and parallelization, resulting in unparalleled performance in NLP tasks. Its ability to process information globally and its adaptability to various tasks make it a pivotal development in the field.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    reduce_prompt = build_summary_prompt(text=text, max_words=500, template=REDUCE_SUMMARY_PROMPT_TPL)\n",
    "    final_summary = get_chat_response(prompt=reduce_prompt, sys_message=SYS_MESSAGE, model=REDUCE_LLM, max_tokens=1024)\n",
    "    logging.info(\"Final Reduce Summary:\\n%s\", final_summary)\n",
    "except Exception as e:\n",
    "    logging.error(f\"An error occured while generating final summary: {e}\")\n",
    "else:\n",
    "    output = {\"final_summary\": final_summary, \"map_summaries\": map_summaries}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
