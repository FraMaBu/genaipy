{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: Summarizing PDFs with Map-Reduce Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "This notebook demoes a workflow leveraging the `Map-Reduce` technique to generate coherent summaries of PDF documents.\n",
    "\n",
    "The workflow consists of three sequential steps:\n",
    "1. **Load and split PDF**: Load the desired PDF and create text chunks for each page.\n",
    "2. **Map-Step**: For each page, generate a concise summary (~150 words) using `GPT-3.5-turbo`.\n",
    "3. **Reduce-Step**: Create final, consolidated summary using `GPT-4-turbo `from set of summaries generated in `Map-Step`.\n",
    "\n",
    "**Key Advantages:**\n",
    "\n",
    "This modular approach, which breaks down a complex task into manageable sub-tasks, offers several advantages over directly generating a summary from the whole PDF:\n",
    "- **Handling of Large Data Volumes**: Enables efficient handling of larger documents by breaking them down into manageable pages, which allows for parallel processing.\n",
    "- **Cost Savings with Cheaper LLMs**: Allows for the use of more cost-effective LLMs for generating intermediate summaries in the `Map-Step`, reducing overall operational costs while maintaining effectiveness.\n",
    "- **Improved Summary Quality**: Each page is summarized individually, leading to more nuanced and accurate summaries, with the flexibility to customize and adjust the process for different use cases.\n",
    "\n",
    "**Disclaimer:**\n",
    "\n",
    "This notebook is strictly for educational purposes. Users should employ it responsibly, ensuring accuracy, respecting privacy and copyright, and avoiding the dissemination of misinformation. Always consider the ethical implications and context of the generated content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load dependencies and set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an inital step, we load the required dependencies and set the parameters for the `Map-Reduce` workflow. In this demo, we summarize the main content of the seminal `\"Attention Is All You Need\"` paper by Google Research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from genaipy.extractors.pdf import extract_pages_text\n",
    "from genaipy.openai_apis.chat import get_chat_response\n",
    "from genaipy.prompts.build_prompt import build_prompt\n",
    "from genaipy.prompts.generate_summaries import SUMMARY_PROMPT_TPL, REDUCE_SUMMARY_PROMPT_TPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('d:/genai/GenAIPy/') # Change directory to root\n",
    "\n",
    "# Logging settings\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# PDF settings\n",
    "PDF_URL = \"demos/data/transformers_attention_paper.pdf\" # https://arxiv.org/abs/1706.03762\n",
    "START = 1\n",
    "END = 10 # exclude references and appendix\n",
    "\n",
    "# LLM settings\n",
    "MAP_LLM = \"gpt-3.5-turbo\"\n",
    "REDUCE_LLM = \"gpt-4-1106-preview\"\n",
    "SYS_MESSAGE = \"You are a generative AI expert. You explain complex AI concepts in simple language so regular users can understand your answers.\"\n",
    "MAP_MAX_WORDS = 150\n",
    "REDUCE_MAX_WORDS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [1/3]: Load and split PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first step, we load the PDF article and split the text content of each page in separate chunks. This enables us to generate an individual summary per page in the following `Map-Step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded text from 10 PDF pages.\n"
     ]
    }
   ],
   "source": [
    "pages = extract_pages_text(pdf_path=PDF_URL, start_page=START, end_page=END)\n",
    "print(f\"Successfully loaded text from {len(pages)} PDF pages.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [2/3]: Map-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, we execute the mapping loop of the `Map-Reduce` technique. Specifically, we generate a summary of each page's text content with `GPT-3.5-turbo` and store the resulting intermediate summaries in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe09ef542d04b28ada0eef3ca080dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating Map Summaries:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 12:17:25,163 - INFO - Successfully completed Chat API request. Total token usage: 906\n",
      "2023-12-13 12:17:25,163 - INFO - Map Summary #1: The text is discussing a new network architecture called the Transformer, which is based solely on attention mechanisms and does not use recurrent or convolutional neural networks. The authors conducted experiments on machine translation tasks and found that the Transformer models outperformed existing models in terms of quality, training time, and parallelizability. The model achieved a BLEU score of 28.4 on the English-to-German translation task and a state-of-the-art BLEU score of 41.8 on the English-to-French translation task. The authors also demonstrated that the Transformer performed well on other tasks, such as English constituency parsing, with both large and limited training data. The paper was published in the 31st Conference on Neural Information Processing Systems (NIPS 2017).\n",
      "2023-12-13 12:17:41,194 - INFO - Successfully completed Chat API request. Total token usage: 1089\n",
      "2023-12-13 12:17:41,196 - INFO - Map Summary #2: The text discusses the concept of the Transformer model architecture, which uses attention mechanisms instead of recurrent networks or convolutions. The Transformer model allows for more parallelization, reducing the constraints of sequential computation. It achieves this by relying on self-attention, which relates different positions within a sequence to compute representations. This approach has been successful in various tasks such as reading comprehension and textual entailment. Unlike other models, the Transformer does not use sequence-aligned RNNs or convolutions. Instead, it uses stacked self-attention and point-wise, fully connected layers in both the encoder and decoder. The Transformer model has shown promising results in translation quality with reduced training time and improved computational efficiency.\n",
      "2023-12-13 12:17:49,111 - INFO - Successfully completed Chat API request. Total token usage: 590\n",
      "2023-12-13 12:17:49,112 - INFO - Map Summary #3: The text describes an AI model called the Transformer, which is used for tasks like language translation. It consists of an encoder and a decoder. The encoder has multiple layers, each with a self-attention mechanism and a feed-forward network. The decoder also has multiple layers and adds an additional sub-layer that performs attention over the encoder's output. The attention function maps a query and key-value pairs to an output by computing a weighted sum of the values based on their compatibility with the query. The model uses residual connections and layer normalization to improve performance. The encoder and decoder layers produce outputs of dimension 512.\n",
      "2023-12-13 12:17:58,635 - INFO - Successfully completed Chat API request. Total token usage: 784\n",
      "2023-12-13 12:17:58,636 - INFO - Map Summary #4: The text explains two important concepts in attention mechanisms: Scaled Dot-Product Attention and Multi-Head Attention. Scaled Dot-Product Attention involves computing dot products between queries and keys, dividing them by a scaling factor, and applying softmax to obtain weights. Multi-Head Attention, on the other hand, projects queries, keys, and values multiple times, performs attention in parallel, and concatenates the output values. The text also discusses the pros and cons of dot-product and additive attention. While dot-product attention is faster and more space-efficient, additive attention outperforms it for larger key dimensions. This is because the dot products can grow large, affecting the softmax function's gradients. To address this, the dot products are scaled by a factor that depends on the key dimension.\n",
      "2023-12-13 12:18:08,377 - INFO - Successfully completed Chat API request. Total token usage: 995\n",
      "2023-12-13 12:18:08,378 - INFO - Map Summary #5: The text explains the concept of multi-head attention in the Transformer model. Multi-head attention allows the model to attend to different parts of the input sequence simultaneously. It combines multiple attention heads, each of which attends to a different subspace of information. This improves the model's ability to capture complex relationships and dependencies in the data. The Transformer uses multi-head attention in three ways: encoder-decoder attention to attend to all positions in the input sequence, self-attention in the encoder and decoder to attend to positions within the same layer, and masking to prevent leftward information flow in the decoder. The model also includes position-wise feed-forward networks and uses embeddings and softmax functions to convert inputs and outputs to vectors and probabilities respectively. Positional encoding is used to incorporate information about the sequence order into the model.\n",
      "2023-12-13 12:18:16,623 - INFO - Successfully completed Chat API request. Total token usage: 1016\n",
      "2023-12-13 12:18:16,624 - INFO - Map Summary #6: The provided text discusses different layer types in AI models, such as self-attention, recurrent, and convolutional layers. It compares their complexity, parallelizability, and ability to learn long-range dependencies. Self-attention layers have a constant number of operations and outperform recurrent layers in terms of computational complexity for shorter sequences. The text also mentions the use of positional encodings for representing input embeddings and compares the effectiveness of fixed and learned encodings. The experiments show that the sinusoidal version of positional encodings allows the model to extrapolate to longer sequence lengths. The summary gives an overview of the main points discussed in the text sample.\n",
      "2023-12-13 12:18:23,305 - INFO - Successfully completed Chat API request. Total token usage: 954\n",
      "2023-12-13 12:18:23,305 - INFO - Map Summary #7: The text describes the use of self-attention and convolutional layers in neural networks. It explains that self-attention allows for more interpretable models and discusses the benefits of convolutional layers. The training regime for the models and the optimizer used are also described. The models were trained on the WMT 2014 English-German and English-French datasets. Regularization techniques, such as dropout, were applied during training. The summary provides an overview of the key points discussed in the text sample.\n",
      "2023-12-13 12:18:32,329 - INFO - Successfully completed Chat API request. Total token usage: 1178\n",
      "2023-12-13 12:18:32,329 - INFO - Map Summary #8: The text discusses the success of the Transformer model in machine translation tasks. The Transformer model achieved better BLEU scores compared to previous state-of-the-art models, while requiring less training cost. The big Transformer model outperformed previously reported models on the English-to-German translation task, establishing a new state-of-the-art BLEU score. On the English-to-French translation task, the big model achieved a BLEU score surpassing previously published models at a fraction of the training cost. The text also mentions the use of label smoothing during training to improve accuracy and BLEU score. Various model variations were evaluated to determine the importance of different components of the Transformer, considering factors such as attention heads and attention key dimensions.\n",
      "2023-12-13 12:18:38,787 - INFO - Successfully completed Chat API request. Total token usage: 1042\n",
      "2023-12-13 12:18:38,787 - INFO - Map Summary #9: The text provides a table comparing different variations of the Transformer architecture. It shows that reducing the attention key size hurts the model's quality and suggests the need for a more sophisticated compatibility function. Bigger models and dropout are found to be beneficial for the model's performance. Additionally, replacing the sinusoidal positional encoding with learned positional embeddings resulted in nearly identical results. The text also mentions the Transformer's ability to generalize well to English constituency parsing, achieving competitive results compared to other methods.\n",
      "2023-12-13 12:18:47,790 - INFO - Successfully completed Chat API request. Total token usage: 1126\n",
      "2023-12-13 12:18:47,790 - INFO - Map Summary #10: The text sample discusses the Transformer model, which is an attention-based sequence transduction model that replaces recurrent layers with multi-headed self-attention. The Transformer model has been trained on various datasets, including the Wall Street Journal portion of the Penn Treebank, and has achieved state-of-the-art results on translation tasks such as English-to-German and English-to-French translations. The model outperforms previous models, except for the Recurrent Neural Network Grammar. The authors of the paper are excited about the future applications of attention-based models and plan to extend the Transformer to handle tasks involving different modalities such as images, audio, and video. The code used to train and evaluate the models is available on GitHub.\n"
     ]
    }
   ],
   "source": [
    "map_summaries = []\n",
    "for page in tqdm(pages, desc=\"Generating Map Summaries\"):\n",
    "    try:\n",
    "        map_prompt = build_prompt(template=SUMMARY_PROMPT_TPL, text=pages[page][\"content\"], max_words=MAP_MAX_WORDS)\n",
    "        summary = get_chat_response(map_prompt, sys_message= SYS_MESSAGE, model=MAP_LLM)\n",
    "        map_summaries.append(summary)\n",
    "        logging.info(\"Map Summary #%d: %s\", page, summary)\n",
    "    except Exception as e:\n",
    "        logging.error(\"An error occured while generating summary #%d: %s\", page, summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step [3/3]: Reduce-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step of the workflow, we combine the individual intermediate summaries, so we can distill them into a cohesive summary in the `Reduce-Step` with `GPT-4-turbo`. As we can see, the resulting final summary gives a comprehensive overview of the paper without omitting important information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is discussing a new network architecture called the Transformer, which is based solely on attention mechanisms and does not use recurrent or convolutional neural networks. The authors conducted experiments on machine translation tasks and found that the Transformer models outperformed existing models in terms of quality, training time, and parallelizability. The model achieved a BLEU score of 28.4 on the English-to-German translation task and a state-of-the-art BLEU score of 41.8 on the English-to-French translation task. The authors also demonstrated that the Transformer performed well on other tasks, such as English constituency parsing, with both large and limited training data. The paper was published in the 31st Conference on Neural Information Processing Systems (NIPS 2017).\n",
      "The text discusses the concept of the Transformer model architecture, which uses attention mechanisms instead of recurrent networks or convolutions. The Transformer model allows for more parallelization, reducing the constraints of sequential computation. It achieves this by relying on self-attention, which relates different positions within a sequence to compute representations. This approach has been successful in various tasks such as reading comprehension and textual entailment. Unlike other models, the Transformer does not use sequence-aligned RNNs or convolutions. Instead, it uses stacked self-attention and point-wise, fully connected layers in both the encoder and decoder. The Transformer model has shown promising results in translation quality with reduced training time and improved computational efficiency.\n",
      "The text describes an AI model called the Transformer, which is used for tasks like language translation. It consists of an encoder and a decoder. The encoder has multiple layers, each with a self-attention mechanism and a feed-forward network. The decoder also has multiple layers and adds an additional sub-layer that performs attention over the encoder's output. The attention function maps a query and key-value pairs to an output by computing a weighted sum of the values based on their compatibility with the query. The model uses residual connections and layer normalization to improve performance. The encoder and decoder layers produce outputs of dimension 512.\n",
      "The text explains two important concepts in attention mechanisms: Scaled Dot-Product Attention and Multi-Head Attention. Scaled Dot-Product Attention involves computing dot products between queries and keys, dividing them by a scaling factor, and applying softmax to obtain weights. Multi-Head Attention, on the other hand, projects queries, keys, and values multiple times, performs attention in parallel, and concatenates the output values. The text also discusses the pros and cons of dot-product and additive attention. While dot-product attention is faster and more space-efficient, additive attention outperforms it for larger key dimensions. This is because the dot products can grow large, affecting the softmax function's gradients. To address this, the dot products are scaled by a factor that depends on the key dimension.\n",
      "The text explains the concept of multi-head attention in the Transformer model. Multi-head attention allows the model to attend to different parts of the input sequence simultaneously. It combines multiple attention heads, each of which attends to a different subspace of information. This improves the model's ability to capture complex relationships and dependencies in the data. The Transformer uses multi-head attention in three ways: encoder-decoder attention to attend to all positions in the input sequence, self-attention in the encoder and decoder to attend to positions within the same layer, and masking to prevent leftward information flow in the decoder. The model also includes position-wise feed-forward networks and uses embeddings and softmax functions to convert inputs and outputs to vectors and probabilities respectively. Positional encoding is used to incorporate information about the sequence order into the model.\n",
      "The provided text discusses different layer types in AI models, such as self-attention, recurrent, and convolutional layers. It compares their complexity, parallelizability, and ability to learn long-range dependencies. Self-attention layers have a constant number of operations and outperform recurrent layers in terms of computational complexity for shorter sequences. The text also mentions the use of positional encodings for representing input embeddings and compares the effectiveness of fixed and learned encodings. The experiments show that the sinusoidal version of positional encodings allows the model to extrapolate to longer sequence lengths. The summary gives an overview of the main points discussed in the text sample.\n",
      "The text describes the use of self-attention and convolutional layers in neural networks. It explains that self-attention allows for more interpretable models and discusses the benefits of convolutional layers. The training regime for the models and the optimizer used are also described. The models were trained on the WMT 2014 English-German and English-French datasets. Regularization techniques, such as dropout, were applied during training. The summary provides an overview of the key points discussed in the text sample.\n",
      "The text discusses the success of the Transformer model in machine translation tasks. The Transformer model achieved better BLEU scores compared to previous state-of-the-art models, while requiring less training cost. The big Transformer model outperformed previously reported models on the English-to-German translation task, establishing a new state-of-the-art BLEU score. On the English-to-French translation task, the big model achieved a BLEU score surpassing previously published models at a fraction of the training cost. The text also mentions the use of label smoothing during training to improve accuracy and BLEU score. Various model variations were evaluated to determine the importance of different components of the Transformer, considering factors such as attention heads and attention key dimensions.\n",
      "The text provides a table comparing different variations of the Transformer architecture. It shows that reducing the attention key size hurts the model's quality and suggests the need for a more sophisticated compatibility function. Bigger models and dropout are found to be beneficial for the model's performance. Additionally, replacing the sinusoidal positional encoding with learned positional embeddings resulted in nearly identical results. The text also mentions the Transformer's ability to generalize well to English constituency parsing, achieving competitive results compared to other methods.\n",
      "The text sample discusses the Transformer model, which is an attention-based sequence transduction model that replaces recurrent layers with multi-headed self-attention. The Transformer model has been trained on various datasets, including the Wall Street Journal portion of the Penn Treebank, and has achieved state-of-the-art results on translation tasks such as English-to-German and English-to-French translations. The model outperforms previous models, except for the Recurrent Neural Network Grammar. The authors of the paper are excited about the future applications of attention-based models and plan to extend the Transformer to handle tasks involving different modalities such as images, audio, and video. The code used to train and evaluate the models is available on GitHub.\n"
     ]
    }
   ],
   "source": [
    "# Process generated map summaries for reduce step\n",
    "\n",
    "string = \"\\n\".join(map_summaries)\n",
    "text = string.replace(\"\\n\\n\", \"\") # clean potential double newlines in joined map summaries\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 12:19:25,903 - INFO - Successfully completed Chat API request. Total token usage: 1919\n",
      "2023-12-13 12:19:25,904 - INFO - Final Reduce Summary:\n",
      "**Overview of the Transformer Model**\n",
      "\n",
      "- The Transformer is a novel AI model created for tasks like language translation.\n",
      "- It is built around attention mechanisms instead of previous methods like recurrent networks or convolutions.\n",
      "- Its architecture consists of an encoder and a decoder, each with multiple layers that facilitate parallel processing and reduce training time.\n",
      "  \n",
      "**Key Features and Mechanisms**\n",
      "\n",
      "- **Self-Attention Layers**: These layers allow each position to attend to all positions in the previous layer and help the model to learn long-range dependencies efficiently.\n",
      "- **Encoder and Decoder**: The encoder processes the input sequence and the decoder generates the translated output, both leveraging self-attention and feed-forward networks.\n",
      "- **Attention Types**:\n",
      "  - *Scaled Dot-Product Attention*: This calculates the relevance of different parts of the input data.\n",
      "  - *Multi-Head Attention*: It extends the dot-product attention by running it in parallel across multiple 'heads', improving the model's capacity to focus on different aspects of the information.\n",
      "\n",
      "**Performance and Training**\n",
      "\n",
      "- The Transformer has exhibited superior performance compared to existing models on English-to-German and English-to-French translation tasks, with higher BLEU scores indicating better translation quality.\n",
      "- It has been trained on standard datasets such as WMT for English-German and English-French and Penn Treebank for English constituency parsing, showcasing its generalization ability.\n",
      "\n",
      "**Innovative Components**\n",
      "\n",
      "- **Positional Encodings**: These are used to maintain sequence order information; both fixed (sinusoidal) and learned positional encodings have shown comparable results.\n",
      "- **Hyperparameters**: Studies on various Transformer configurations highlight the importance of adequate attention head size and key dimensions.\n",
      "\n",
      "**Impact and Future Applications**\n",
      "\n",
      "- The success of the Transformer indicates a shift towards attention-based models.\n",
      "- It demonstrates potential for extension to other data modalities, such as image and audio.\n",
      "- Code and methods are publicly available, fostering further research and development.\n",
      "\n",
      "**Advantages**\n",
      "\n",
      "- Improved parallelizability and computational efficiency.\n",
      "- Effective learning of dependencies regardless of sequence length.\n",
      "- More interpretable than convolutional and recurrent layers.\n",
      "\n",
      "In conclusion, the Transformer's attention-centric design, coupled with its compelling performance and efficiency, has set a new standard in neural network architectures for sequence transduction tasks such as language translation. Its ability to generalize and potential applicability to a wide range of modalities make it a promising model for future advancements in AI.\n"
     ]
    }
   ],
   "source": [
    "# Execute reduce step to generate final summary\n",
    "\n",
    "try:\n",
    "    reduce_prompt = build_prompt(template=REDUCE_SUMMARY_PROMPT_TPL, text=text, max_words=REDUCE_MAX_WORDS)\n",
    "    final_summary = get_chat_response(prompt=reduce_prompt, sys_message=SYS_MESSAGE, model=REDUCE_LLM, max_tokens=1024)\n",
    "    logging.info(\"Final Reduce Summary:\\n%s\", final_summary)\n",
    "except Exception as e:\n",
    "    logging.error(\"An error occured while generating final summary: %s\", e)\n",
    "else:\n",
    "    output = {\"final_summary\": final_summary, \"map_summaries\": map_summaries}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genaipy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
